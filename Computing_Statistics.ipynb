{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to have 'books.data' in your directory! It will be imported as 'books_df'\n",
    "import pickle\n",
    "with open('books.data', 'rb') as filehandle:  \n",
    "    books_df = pickle.load(filehandle)\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open('word_len.data', 'rb') as filehandle:  \n",
    "    word_mean = pickle.load(filehandle)\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "ratings_csv = csv.reader(open('ratings.txt'))\n",
    "ratings = []\n",
    "for row in ratings_csv:\n",
    "    ratings.append(row[0])\n",
    "ratings = list(map(lambda x: float(x), ratings))\n",
    "ratings\n",
    "\n",
    "\n",
    "books_stats = {'tokens':books_df,'ratings':ratings,'pos_perc':pos_perc,'neg_perc':neg_perc,\n",
    "              'word_count':word_count,'ave_len':ave_len}\n",
    "\n",
    "\n",
    "books_stats['means'][:3]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "books_stats['ratings'] = pd.Series(books_stats['ratings'])\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "pos_csv = csv.reader(open('positive-words.txt'))\n",
    "neg_csv = csv.reader(open('negative-words.txt'))\n",
    "pos = []\n",
    "neg = []\n",
    "for row in pos_csv:\n",
    "    pos.append(row[0])\n",
    "for row in neg_csv:\n",
    "    neg.append(row[0])\n",
    "\n",
    "\n",
    "\n",
    "pos[:5]\n",
    "neg[:5]\n",
    "for s in pos:\n",
    "    if s in neg:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "for i in range(len(books_stats['tokens'])):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    books_stats['tokens'][i]['positive'] = pd.Series(map(lambda x: 1 if x in pos else 0, \n",
    "                        list(books_stats['tokens'][i]['full'])), index = books_stats['tokens'][i].index)\n",
    "    books_stats['tokens'][i]['negative'] = pd.Series(map(lambda x: 1 if x in neg else 0, \n",
    "                        list(books_stats['tokens'][i]['full'])), index = books_stats['tokens'][i].index)\n",
    "\n",
    "\n",
    "\n",
    "list(books_stats['tokens'][0].index)\n",
    "\n",
    "pd.Series(map(lambda x: 1 if (x in pos) else 0, list(books_stats['tokens'][0]['word'])),\n",
    "          index = books_stats['tokens'][0].index)\n",
    "\n",
    "\n",
    "books_stats['tokens'][0]\n",
    "\n",
    "for i in range(len(books_stats['tokens'])):\n",
    "    books_stats['tokens'][i]['full'] = pd.Series(map(lambda x: str(x).lower(), list(books_stats['tokens'][i].index)),\n",
    "                                                 index = books_stats['tokens'][i].index)\n",
    "\n",
    "\n",
    "\n",
    "word_count = pd.Series(map(lambda x: len(x), books_stats['tokens']))\n",
    "pos_count = pd.Series(map(lambda i: sum(books_stats['tokens'][i]['positive']), range(len(books_stats['tokens']))))\n",
    "neg_count = pd.Series(map(lambda i: sum(books_stats['tokens'][i]['negative']), range(len(books_stats['tokens']))))\n",
    "pos_perc = pd.Series(map(lambda i: pos_count[i]/word_count[i], range(len(books_stats['tokens']))))\n",
    "neg_perc = pd.Series(map(lambda i: neg_count[i]/word_count[i], range(len(books_stats['tokens']))))\n",
    "\n",
    "\n",
    "\n",
    "sum(pos_perc < neg_perc)\n",
    "\n",
    "temp_arr = np.array([books_stats['ratings'],pos_perc,neg_perc])\n",
    "\n",
    "temp_arr = temp_arr[:,temp_arr[0,:] != 0]\n",
    "\n",
    "np.mean(temp_arr[0,temp_arr[1,:] < temp_arr[2,:]])\n",
    "\n",
    "np.mean(temp_arr[0,temp_arr[1,:] > temp_arr[2,:]])\n",
    "\n",
    "import pickle\n",
    "with open('books_stats.data', 'wb') as filehandle:  \n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(books_stats, filehandle)\n",
    "\n",
    "\n",
    "\n",
    "books_stats.keys()\n",
    "\n",
    "for i in range(len(books_stats['tokens'])):\n",
    "    books_stats['tokens'][i]['length'] = pd.Series(map(lambda x: len(x), books_stats['tokens'][i]['full'])\n",
    "                                                  , index = books_stats['tokens'][i].index)\n",
    "\n",
    "\n",
    "\n",
    "ave_len = pd.Series(map(lambda x: books_stats['tokens'][x]['length'].mean(), range(len(books_stats['tokens']))))\n",
    "\n",
    "\n",
    "books_stats['ave_len']\n",
    "\n",
    "pos_perc\n",
    "\n",
    "########## Basic model ############\n",
    "X = np.array(pd.DataFrame([word_count,ave_len,pos_perc,neg_perc])).transpose()\n",
    "Y = np.array(books_stats['ratings'])\n",
    "\n",
    "\n",
    "X = X[Y != 0,:]\n",
    "Y = Y[Y != 0]\n",
    "Y = np.array( list(map(lambda y: 1 if y > 4 else 0, Y)) )\n",
    "\n",
    "\n",
    "idx = np.random.permutation(len(Y))\n",
    "t = int(.8*len(Y))\n",
    "X_train = X[idx[:t],:]\n",
    "X_test = X[idx[t:],:]\n",
    "Y_train = Y[idx[:t]]\n",
    "Y_test = Y[idx[t:]]\n",
    "\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 6174)\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, Y_train);\n",
    "\n",
    "\n",
    "\n",
    "Y_train_pred = rf.predict(X_train)\n",
    "Y_test_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_test_pred, pos_label=1)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (15,12))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "sum(Y_test)\n",
    "\n",
    "\n",
    "sum((Y_train_pred - Y_train)**2)/len(Y_train)\n",
    "sum((np.mean(Y_train) - Y_train)**2)/len(Y_train)\n",
    "sum((Y_test_pred - Y_test)**2)/len(Y_test)\n",
    "sum((np.mean(Y_test) - Y_test)**2)/len(Y_test)\n",
    "\n",
    "\n",
    "\n",
    "Y_train_pred\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(thing, min_count = 1, size = 100, window = 10)\n",
    "\n",
    "import pickle\n",
    "with open('w2v_model.data', 'wb') as filehandle:  \n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(w2v_model, filehandle)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "thing[1]\n",
    "\n",
    "w2v_model.wv.vocab\n",
    "\n",
    "w2v_model.similarity('alic','wonderland')\n",
    "\n",
    "\n",
    "\n",
    "len(books_stats['tokens'][0])\n",
    "\n",
    "((1136537/2571.0)*3.41)/60\n",
    "\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "\n",
    "\n",
    "gutenberg.words(gutenberg.fileids()[0])[-1]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
